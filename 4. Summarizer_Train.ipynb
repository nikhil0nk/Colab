{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Summarizer_Train.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"nheRFz12BXKb"},"source":["!nvidia-smi\n","!pip install happytransformer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qGPxV7K1BZgt"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir(\"/content/drive/MyDrive/Summarizer\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DA65CiMXAH0n"},"source":["\"\"\"\n","Script for fine-tuning Pegasus\n"," \n","Reference:\n","  https://huggingface.co/transformers/master/custom_datasets.html\n","\n","\"\"\"\n","\n","from transformers import PegasusForConditionalGeneration, PegasusTokenizer, Trainer, TrainingArguments\n","import torch\n","\n","\n","class PegasusDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels['input_ids'][idx])  # torch.tensor(self.labels[idx])\n","        return item\n","    def __len__(self):\n","        return len(self.labels['input_ids'])  # len(self.labels)\n","\n","      \n","def prepare_data(model_name, \n","                 train_texts, train_labels, \n","                 val_texts=None, val_labels=None, \n","                 test_texts=None, test_labels=None):\n","  \"\"\"\n","  Prepare input data for model fine-tuning\n","  \"\"\"\n","  tokenizer = PegasusTokenizer.from_pretrained(model_name)\n","\n","  prepare_val = False if val_texts is None or val_labels is None else True\n","  prepare_test = False if test_texts is None or test_labels is None else True\n","\n","  def tokenize_data(texts, labels):\n","    encodings = tokenizer(texts, truncation=True, padding=True)\n","    decodings = tokenizer(labels, truncation=True, padding=True)\n","    dataset_tokenized = PegasusDataset(encodings, decodings)\n","    return dataset_tokenized\n","\n","  train_dataset = tokenize_data(train_texts, train_labels)\n","  val_dataset = tokenize_data(val_texts, val_labels) if prepare_val else None\n","  test_dataset = tokenize_data(test_texts, test_labels) if prepare_test else None\n","\n","  return train_dataset, val_dataset, test_dataset, tokenizer\n","\n","\n","def prepare_fine_tuning(model_name, tokenizer, train_dataset, val_dataset=None, freeze_encoder=False, output_dir='./results'):\n","  \"\"\"\n","  Prepare configurations and base model for fine-tuning\n","  \"\"\"\n","  torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","  model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n","\n","  if freeze_encoder:\n","    for param in model.model.encoder.parameters():\n","      param.requires_grad = False\n","\n","  if val_dataset is not None:\n","    training_args = TrainingArguments(\n","      output_dir=\"/content/drive/MyDrive/Summarizer/pegasus\",           # output directory\n","      num_train_epochs=3,           # total number of training epochs\n","      per_device_train_batch_size=1,   # batch size per device during training, can increase if memory allows\n","      per_device_eval_batch_size=1,    # batch size for evaluation, can increase if memory allows\n","      save_steps=500,                  # number of updates steps before checkpoint saves\n","      save_total_limit=5,              # limit the total amount of checkpoints and deletes the older checkpoints\n","      evaluation_strategy='steps',     # evaluation strategy to adopt during training\n","      eval_steps=100,                  # number of update steps before evaluation\n","      warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","      weight_decay=0.01,               # strength of weight decay\n","      logging_dir='./logs',            # directory for storing logs\n","      logging_steps=10,\n","    )\n","\n","    trainer = Trainer(\n","      model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n","      args=training_args,                  # training arguments, defined above\n","      train_dataset=train_dataset,         # training dataset\n","      eval_dataset=val_dataset,            # evaluation dataset\n","      tokenizer=tokenizer\n","    )\n","\n","  else:\n","    training_args = TrainingArguments(\n","      output_dir=\"/content/drive/MyDrive/Summarizer/pegasus\",           # output directory\n","      num_train_epochs=3,           # total number of training epochs\n","      per_device_train_batch_size=1,   # batch size per device during training, can increase if memory allows\n","      save_steps=5000,                  # number of updates steps before checkpoint saves\n","      save_total_limit=5,              # limit the total amount of checkpoints and deletes the older checkpoints\n","      warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","      weight_decay=0.01,               # strength of weight decay\n","      logging_dir='./logs',            # directory for storing logs\n","      logging_steps=500,\n","    )\n","\n","    trainer = Trainer(\n","      model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n","      args=training_args,                  # training arguments, defined above\n","      train_dataset=train_dataset,         # training dataset\n","      tokenizer=tokenizer\n","    )\n","\n","  return trainer\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yrs6AkXTAYUT"},"source":["from datasets import load_dataset\n","import pandas as pd\n","\n","data = pd.read_csv(\"train.csv\")\n","train_texts = []\n","train_labels = []\n","for r,i in data.iterrows():\n","  train_texts.append(data['input'][r])\n","  train_labels.append(data['target'][r])\n","\n","model_name = 'tuner007/pegasus_summarizer'\n","train_dataset, _, _, tokenizer = prepare_data(model_name, train_texts, train_labels)\n","trainer = prepare_fine_tuning(model_name, tokenizer, train_dataset)\n","trainer.train()"],"execution_count":null,"outputs":[]}]}